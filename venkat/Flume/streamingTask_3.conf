agent.sources = EXEC
agent.channels = Filechannel1 Filechannel2 Filechannel3
agent.sinks = HDFS HIVE HBASE

agent.sources.EXEC.type = exec
agent.sources.EXEC.command = tail -F /tmp/productlog.csv
agent.sources.EXEC.channels = Filechannel1
agent.sources.EXEC.channels = Filechannel2
agent.sources.EXEC.channels = Filechannel3

agent.sinks.HDFS.channel = Filechannel1
agent.sinks.HDFS.type = hdfs
agent.sinks.HDFS.hdfs.path = hdfs://localhost:8020/user/orienit/flume/tweets2
agent.sinks.HDFS.hdfs.fileType = DataStream
agent.sinks.HDFS.hdfs.writeFormat = Text
agent.sinks.HDFS.hdfs.batchSize = 1000
agent.sinks.HDFS.hdfs.rollSize = 0
agent.sinks.HDFS.hdfs.rollCount = 10000
agent.sinks.HDFS.hdfs.useLocalTimeStamp = true

agent.sinks.HIVE.type = hive
agent.sinks.HIVE.hive.metastore = thrift://localhost:9083
agent.sinks.HIVE.hive.database = kalyan
agent.sinks.HIVE.hive.table = users2
agent.sinks.HIVE.serializer = DELIMITED
agent.sinks.HIVE.serializer.delimiter = ","
agent.sinks.HIVE.serializer.fieldnames=userid,username,email,device,status,country,state,city,dt
agent.sinks.HIVE.channel = Filechannel2

agent.sinks.HBASE.type = hbase
agent.sinks.HBASE.table = users4
agent.sinks.HBASE.columnFamily = cf
agent.sinks.HBASE.serializer = com.orienit.kalyan.flume.sink.CsvHbaseEventSerializer
agent.sinks.HBASE.serializer.delimiter = ,
agent.sinks.HBASE.serializer.colNames = userid,username,email,device,status,country,state,city,dt
agent.sinks.HBASE.channel = Filechannel3

agent.channels.Filechannel1.type = memory
agent.channels.Filechannel1.capacity = 1000
agent.channels.Filechannel1.transactionCapacity = 100

agent.channels.Filechannel2.type = memory
agent.channels.Filechannel2.capacity = 1000
agent.channels.Filechannel2.transactionCapacity = 100

agent.channels.Filechannel3.type = memory
agent.channels.Filechannel3.capacity = 1000
agent.channels.Filechannel3.transactionCapacity = 100


streamingTask_5.conf

---------------------------------------------------------

Hive commands:
--------------
create table::
-------------
create table users2(
userid BIGINT,
username string,
email string,
device string,
status string,
country string,
state string,
city string,
dt string)
clustered by (userid) into 5 buckets stored as orc;



start metastore:
-----------------
sudo service hive-metastore start



hbase commands:
--------------

1.create 'users4','cf'

2.list

3.scan 'users3'

flume command::
---------------

flume-ng agent --conf /usr/lib/flume-ng/conf/ -f /usr/lib/flume-ng/conf/test3.conf -Dflume.root.logger=DEBUG,console -n agent