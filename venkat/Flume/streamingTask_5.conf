agent.sources = EXEC
agent.channels = Filechannel1 Filechannel2 Filechannel3 Filechannel4
agent.sinks = KAFKA HDFS HIVE HBASE

agent.sources.EXEC.type = exec
agent.sources.EXEC.command = tail -F /tmp/productlog.csv
agent.sources.EXEC.channels = Filechannel1
agent.sources.EXEC.channels = Filechannel2
agent.sources.EXEC.channels = Filechannel3
agent.sources.EXEC.channels = Filechannel4

agent.sinks.KAFKA.type = org.apache.flume.sink.kafka.KafkaSink
agent.sinks.KAFKA.channel = Filechannel1
agent.sinks.KAFKA.kafka.topic = flume-csv-topic
agent.sinks.KAFKA.kafka.bootstrap.servers = localhost:9092
agent.sinks.KAFKA.kafka.flumeBatchSize = 10


agent.sinks.HDFS.channel = Filechannel2
agent.sinks.HDFS.type = hdfs
agent.sinks.HDFS.hdfs.path = hdfs://localhost:8020/user/orienit/flume/tweets1
agent.sinks.HDFS.hdfs.fileType = DataStream
agent.sinks.HDFS.hdfs.writeFormat = Text
agent.sinks.HDFS.hdfs.batchSize = 1000
agent.sinks.HDFS.hdfs.rollSize = 0
agent.sinks.HDFS.hdfs.rollCount = 10000
agent.sinks.HDFS.hdfs.useLocalTimeStamp = true

agent.sinks.HIVE.type = hive
agent.sinks.HIVE.hive.metastore = thrift://localhost:9083
agent.sinks.HIVE.hive.database = kalyan
agent.sinks.HIVE.hive.table = users1
agent.sinks.HIVE.serializer = DELIMITED
agent.sinks.HIVE.serializer.delimiter = ","
agent.sinks.HIVE.serializer.fieldnames=userid,username,email,device,status,country,state,city,dt
agent.sinks.HIVE.channel = Filechannel3

agent.sinks.HBASE.type = hbase
agent.sinks.HBASE.table = users3
agent.sinks.HBASE.columnFamily = cf
agent.sinks.HBASE.serializer = com.orienit.kalyan.flume.sink.CsvHbaseEventSerializer
agent.sinks.HBASE.serializer.delimiter = ,
agent.sinks.HBASE.serializer.colNames = userid,username,email,device,status,country,state,city,dt
agent.sinks.HBASE.channel = Filechannel4

agent.channels.Filechannel1.type = memory
agent.channels.Filechannel1.capacity = 1000
agent.channels.Filechannel1.transactionCapacity = 100

agent.channels.Filechannel2.type = memory
agent.channels.Filechannel2.capacity = 1000
agent.channels.Filechannel2.transactionCapacity = 100

agent.channels.Filechannel3.type = memory
agent.channels.Filechannel3.capacity = 1000
agent.channels.Filechannel3.transactionCapacity = 100

agent.channels.Filechannel4.type = memory
agent.channels.Filechannel4.capacity = 1000
agent.channels.Filechannel4.transactionCapacity = 100
streamingTask_5.conf

---------------------------------------------------------

Kafka Commands
----------------------------------
1. Creating Kafka Topic:
----------------------------
kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic 

2. List of Kafka Topics:
----------------------------
kafka-topics --list --zookeeper localhost:2181

3.Configure the Kafka Consumer:
----------------------------
kafka-console-consumer --bootstrap-server localhost:9092 --topic test --from-beginning

Hive commands:
--------------
create table::
-------------
create table users1(
userid BIGINT,
username string,
email string,
device string,
status string,
country string,
state string,
city string,
dt string)
clustered by (userid) into 5 buckets stored as orc;



start metastore:
-----------------
sudo service hive-metastore start



hbase commands:
--------------

1.create 'users3','cf'

2.list

3.scan 'users3'

flume command::
---------------

flume-ng agent --conf /usr/lib/flume-ng/conf/ -f /usr/lib/flume-ng/conf/streamingTask_5.conf -Dflume.root.logger=DEBUG,console -n agent