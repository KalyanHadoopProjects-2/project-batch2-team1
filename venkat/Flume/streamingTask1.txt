agent.sources = Twitter
agent.channels = MemChannel1 MemChannel2
agent.sinks =HDFS MongoDB

agent.sources.Twitter.type = com.orienit.kalyan.flume.source.KalyanTwitterSource
agent.sources.Twitter.channels = MemChannel1
agent.sources.Twitter.channels = MemChannel2
agent.sources.Twitter.consumerKey = KjmrdExoo8lDZRXGoX79sWon
agent.sources.Twitter.consumerSecret = 4gUFCA3HWFQHwoKn1AFCfrgnhQ7Uy9NeLisLVmQG96BsZcHev
agent.sources.Twitter.accessToken =  93156255863643408-EQwfBQdJLw7W8I0EiyOjvQYcpLbbTI3
agent.sources.Twitter.accessTokenSecret = JXnyeedbJd3fH0USzP3hdwO7WK6Xm9dnTuCbCsKqFwOo
agent.sources.Twitter.keywords = hadoop, big data, analytics, bigdata, cloudera, data science, data
scientiest, business intelligence, mapreduce, data warehouse, data warehousing, mahout, hbase,
nosql, newsql, businessintelligence, cloudcomputing

agent.channels.MemChannel1.type = memory
agent.channels.MemChannel1.capacity = 1000
agent.channels.MemChannel1.transactionCapacity = 100

agent.channels.MemChannel2.type = memory
agent.channels.MemChannel2.capacity = 1000
agent.channels.MemChannel2.transactionCapacity = 100

agent.sinks.HDFS.channel = MemChannel1
agent.sinks.HDFS.type = hdfs
agent.sinks.HDFS.hdfs.path = hdfs://localhost:8020/user/orienit/flume/tweets1
agent.sinks.HDFS.hdfs.fileType = DataStream
agent.sinks.HDFS.hdfs.writeFormat = Json
agent.sinks.HDFS.hdfs.batchSize = 1000
agent.sinks.HDFS.hdfs.rollSize = 0
agent.sinks.HDFS.hdfs.rollCount = 10000
agent.sinks.HDFS.hdfs.useLocalTimeStamp = true

agent.sinks.MongoDB.type = com.orienit.kalyan.flume.sink.KalyanMongoSink
agent.sinks.MongoDB.hostNames = localhost
agent.sinks.MongoDB.database = flume1
agent.sinks.MongoDB.collection = twitter
agent.sinks.MongoDB.batchSize = 10


agent.sinks.HDFS.channel=Memchannel1
agent.sinks.MongoDB.channel=Memchannel2


flume-ng agent --conf /usr/lib/flume-ng/conf/ -f /usr/lib/flume-ng/conf/streamingTask_1.conf -Dflume.root.logger=DEBUG,console -n agent
